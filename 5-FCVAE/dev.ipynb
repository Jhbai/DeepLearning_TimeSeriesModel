{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1954b33-f9f8-48ba-9454-45b23419ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77ccba6a-f34d-4550-b274-1cae055b560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        super(attention, self).__init__()\n",
    "        self.k_w = nn.Linear(n_dim, n_dim)\n",
    "        self.q_w = nn.Linear(n_dim, n_dim)\n",
    "        self.v_w = nn.Linear(n_dim, n_dim)\n",
    "        self.scale = n_dim**(-.5)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1) #x:(N, W, 1) -> (N, 1, W)\n",
    "        k = self.k_w(x) #k: (N, 1, W)\n",
    "        q = self.q_w(x) #q: (N, 1, W)\n",
    "        v = self.v_w(x) #v: (N, 1, W)\n",
    "        weight = torch.softmax(torch.matmul(k.permute(0, 2, 1), q)*(self.scale), dim = 1) #k.permute(0, 2, 1):(N, W, 1), q:(N, 1, W) -> (N, W, W)\n",
    "        out = torch.matmul(v, weight)\n",
    "        return out\n",
    "\n",
    "class GFM(nn.Module):\n",
    "    def __init__(self, n_dim, n_hid):\n",
    "        super(GFM, self).__init__()\n",
    "        self.dense = nn.Linear(n_dim, n_hid) # (N, W) -> (N, d)\n",
    "    def forward(self, x):\n",
    "        out = torch.fft.fft(x).real # (N, W) -> (N, W)\n",
    "        out = nn.ReLU()(self.dense(out)) # (N, W) -> (N, d)\n",
    "        out = nn.Dropout(p=.1)(out) # (N, d)\n",
    "        return out\n",
    "\n",
    "class LFM(nn.Module):\n",
    "    def __init__(self, n_dim, n_hid1, n_hid2, n_seq):\n",
    "        super(LFM, self).__init__()\n",
    "        self.dense = nn.Linear(n_seq, n_hid1)\n",
    "        self.attn = attention(n_hid1)\n",
    "        self.ffd = nn.Linear((n_dim//n_seq)*n_hid1, n_hid2)\n",
    "\n",
    "        # parameters setup\n",
    "        self.n_dim, self.n_seq, self.n_hid1, self.n_hid2 = n_dim, n_seq, n_hid1, n_hid2\n",
    "    def forward(self, x):\n",
    "        out = x.reshape(x.shape[0], self.n_dim//self.n_seq, self.n_seq) # (N, W) -> (N, n, k)\n",
    "        out = torch.fft.fft(out).real # (N, n, k)\n",
    "        out = self.dense(out).reshape(out.shape[0], self.n_hid1, -1) # (N, n, k) -> (N, n, l) -> (N, l, n)\n",
    "        out = self.attn(out).reshape(out.shape[0], -1) # (N, l, n) -> (N, l, n) -> (N, n*l)\n",
    "        out = nn.ReLU()(self.ffd(out)) # (N, n*l) -> (N, d)\n",
    "        return out\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, n_hid2, n_dim):\n",
    "        n_hid = n_hid2*2 + n_dim\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.Encoder = nn.Sequential(\n",
    "            nn.Linear(n_hid, n_hid//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_hid//2, n_hid//4),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        # Variational Inference\n",
    "        self.mean = nn.Linear(n_hid//4, n_hid//8)\n",
    "        self.logvar = nn.Linear(n_hid//4, n_hid//8)\n",
    "\n",
    "        # Decoder\n",
    "        self.Decoder = nn.Sequential(\n",
    "            nn.Linear(n_hid//8 + n_hid2*2, n_hid//4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(n_hid//4, n_hid//2),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        # Variational Inference\n",
    "        self.recon_mean = nn.Linear(n_hid//2, n_dim)\n",
    "        self.recon_logvar = nn.Linear(n_hid//2, n_dim)\n",
    "\n",
    "    def forward(self, x, LF, GF):\n",
    "        # Encoder\n",
    "        out = self.Encoder(x)\n",
    "        \n",
    "        # Variational Inference\n",
    "        mean = self.mean(out)\n",
    "        logvar = self.logvar(out)\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        z = torch.randn_like(std)*std + mean\n",
    "\n",
    "        # Decoder\n",
    "        z = torch.cat((z, LF, GF), dim = 1)\n",
    "        out = self.Decoder(z)\n",
    "\n",
    "        # Variational Inference\n",
    "        mean = self.recon_mean(out)\n",
    "        logvar = self.recon_logvar(out)\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        x_hat = torch.randn_like(std)*std + mean\n",
    "        return x_hat\n",
    "\n",
    "class FCVAE(nn.Module):\n",
    "    def __init__(self, n_dim, n_hid1, n_hid2, n_seq):\n",
    "        super(FCVAE, self).__init__()\n",
    "        # Parameter Setup\n",
    "        self.n_dim, self.n_hid1, self.n_hid2, self.n_seq = n_dim, n_hid1, n_hid2, n_seq\n",
    "\n",
    "        # Layer Setup\n",
    "        self.GFM = GFM(n_dim, n_hid2)\n",
    "        self.LFM = LFM(n_dim, n_hid1, n_hid2, n_seq)\n",
    "        self.VAE = VAE(n_hid2, n_dim)\n",
    "    def forward(self, x):\n",
    "        gf = self.GFM(x)\n",
    "        lf = self.LFM(x)\n",
    "        out = torch.cat((x, gf, lf), dim = 1)\n",
    "        return self.VAE(out, gf, lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09ef6e56-56a5-4d9a-81d8-511a693317ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 50])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq, n_hid1, n_hid2, n_dim = 10, 20, 30, 50\n",
    "model = FCVAE(n_dim, n_hid1, n_hid2, n_seq)\n",
    "X = torch.randn(500, n_dim)\n",
    "model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9756131c-dd91-4ccf-bf5f-cc611b551f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_seq, n_hid1, n_hid2, n_dim = 10, 20, 30, 50\n",
    "\n",
    "# attn = attention(n_dim)\n",
    "# gfm = GFM(n_dim, n_hid2)\n",
    "# lfm = LFM(n_dim, n_hid1, n_hid2, n_seq)\n",
    "# vae = VAE(10, 20)\n",
    "\n",
    "# X = torch.randn(100, n_dim)\n",
    "# print(X.shape)\n",
    "# print(gfm(X).shape)\n",
    "# print(lfm(X).shape)\n",
    "\n",
    "# print(\"*\"*100)\n",
    "# test = torch.randn(100, 40)\n",
    "# gf, lf = torch.randn(100, 10), torch.randn(100, 10)\n",
    "# print(test.shape)\n",
    "# print(vae(test, gf, lf).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45383f1f-fbe5-45a4-b2d3-24b4e4423825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
